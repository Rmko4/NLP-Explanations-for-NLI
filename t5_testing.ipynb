{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "# Parameters to be defined\n",
    "model_checkpoint = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(model_checkpoint, model_max_length=512)\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The<extra_id_0> walks in<extra_id_1> park</s>\n",
      "<extra_id_0> cute dog<extra_id_1> the<extra_id_2></s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.586868286132812"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "decoded_input = tokenizer.decode(input_ids[0])\n",
    "decoded_labels = tokenizer.decode(labels[0])\n",
    "\n",
    "print(decoded_input)\n",
    "print(decoded_labels)\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(input_ids=input_ids, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> The dog is walking in the park.</s>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(input_ids, max_length=100, num_beams=5, early_stopping=True)\n",
    "\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset esnli (C:/Users/Remco/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e584e8695040df9d46861bc2a142b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "datasets = load_dataset(\"esnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7163270711898804"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following 2 hyperparameters are task-specific\n",
    "max_source_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "# Suppose we have the following 2 training examples:\n",
    "input_sequence_1 = \"Welcome to NYC\"\n",
    "output_sequence_1 = \"Bienvenue à NYC\"\n",
    "\n",
    "input_sequence_2 = \"HuggingFace is a company\"\n",
    "output_sequence_2 = \"HuggingFace est une entreprise\"\n",
    "\n",
    "# encode the inputs\n",
    "task_prefix = \"translate English to French: \"\n",
    "input_sequences = [input_sequence_1, input_sequence_2]\n",
    "\n",
    "encoding = tokenizer(\n",
    "    [task_prefix + sequence for sequence in input_sequences],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_source_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "# encode the targets\n",
    "target_encoding = tokenizer(\n",
    "    [output_sequence_1, output_sequence_2],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_target_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "labels = target_encoding.input_ids\n",
    "\n",
    "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "# forward pass\n",
    "# No need to give right shift token ids as the model automatically creates them.\n",
    "# And tokenizer add EOS token at the end of the sequence\n",
    "loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocess_function(tokenizer, training=True, max_source_length=512, max_target_length=128):\n",
    "    def _preprocess_fn(examples):\n",
    "        input_text = ['premise: ' + premise + ' \\n ' + 'hypothesis: ' + hypothesis\n",
    "                      for premise, hypothesis in zip(examples['premise'], examples['hypothesis'])]\n",
    "\n",
    "        model_inputs = tokenizer(input_text, truncation=True, max_length=max_source_length)\n",
    "\n",
    "        if training:\n",
    "            target_text = examples['explanation_1']\n",
    "            targets = tokenizer(target_text, truncation=True, max_length=max_target_length)\n",
    "\n",
    "            model_inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "        else:\n",
    "            for i in range(1, 4):\n",
    "                key_explanation = 'explanation_' + str(i)\n",
    "                target_text = examples[key_explanation]\n",
    "                targets = tokenizer(target_text, truncation=True, max_length=max_target_length)\n",
    "                model_inputs[key_explanation] = targets[\"input_ids\"]\n",
    "\n",
    "\n",
    "        return model_inputs\n",
    "    return _preprocess_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Remco\\.cache\\huggingface\\datasets\\esnli\\plain_text\\0.0.2\\a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc\\cache-d67e13d575ddf396.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Remco\\.cache\\huggingface\\datasets\\esnli\\plain_text\\0.0.2\\a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc\\cache-eb5e9201f9aa7afa.arrow\n"
     ]
    }
   ],
   "source": [
    "raw_dataset_columns = datasets['train'].column_names\n",
    "\n",
    "preprocess_function_train = get_preprocess_function(tokenizer, training=True)\n",
    "datasets['train'] = datasets['train'].map(preprocess_function_train, batched=True, remove_columns=raw_dataset_columns)\n",
    "\n",
    "preprocess_function_test = get_preprocess_function(tokenizer, training=False)\n",
    "datasets['validation'] = datasets['validation'].map(preprocess_function_test, batched=True, remove_columns=raw_dataset_columns)\n",
    "datasets['test'] = datasets['test'].map(preprocess_function_test, batched=True, remove_columns=raw_dataset_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explanation_1': [37, 12, 281, 6307, 164, 59, 36, 45, 3074, 5, 1],\n",
       " 'explanation_2': [1142,\n",
       "  250,\n",
       "  192,\n",
       "  887,\n",
       "  33,\n",
       "  3,\n",
       "  24209,\n",
       "  6,\n",
       "  405,\n",
       "  59,\n",
       "  1243,\n",
       "  79,\n",
       "  33,\n",
       "  14033,\n",
       "  5,\n",
       "  2759,\n",
       "  887,\n",
       "  24,\n",
       "  33,\n",
       "  3,\n",
       "  24209,\n",
       "  33,\n",
       "  59,\n",
       "  6539,\n",
       "  18233,\n",
       "  3896,\n",
       "  23281,\n",
       "  5,\n",
       "  1],\n",
       " 'explanation_3': [2759,\n",
       "  887,\n",
       "  103,\n",
       "  59,\n",
       "  43,\n",
       "  12,\n",
       "  36,\n",
       "  14033,\n",
       "  5,\n",
       "  3,\n",
       "  17467,\n",
       "  3738,\n",
       "  53,\n",
       "  405,\n",
       "  59,\n",
       "  1243,\n",
       "  18233,\n",
       "  3896,\n",
       "  23281,\n",
       "  5,\n",
       "  37,\n",
       "  887,\n",
       "  103,\n",
       "  59,\n",
       "  43,\n",
       "  12,\n",
       "  43,\n",
       "  131,\n",
       "  2369,\n",
       "  3074,\n",
       "  12,\n",
       "  36,\n",
       "  3,\n",
       "  24209,\n",
       "  5,\n",
       "  1],\n",
       " 'input_ids': [3,\n",
       "  17398,\n",
       "  10,\n",
       "  2759,\n",
       "  887,\n",
       "  33,\n",
       "  3,\n",
       "  24209,\n",
       "  298,\n",
       "  3609,\n",
       "  12,\n",
       "  281,\n",
       "  6307,\n",
       "  5,\n",
       "  22455,\n",
       "  10,\n",
       "  37,\n",
       "  14033,\n",
       "  33,\n",
       "  18233,\n",
       "  3896,\n",
       "  23281,\n",
       "  298,\n",
       "  3609,\n",
       "  12,\n",
       "  281,\n",
       "  6307,\n",
       "  227,\n",
       "  131,\n",
       "  3182,\n",
       "  3074,\n",
       "  5,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['validation'][0]\n",
    "\n",
    "print(tokenizer.decode(datasets['train'][0]['input_ids']))\n",
    "print(tokenizer.decode(datasets['train'][0]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Use data collator to create batched data. This will pad the inputs and labels to the maximum length of the batch.\n",
    "# Might be more efficient to pad to fixed length.\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True, label_pad_token_id=-100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\transformers\\tokenization_utils_base.p</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">y</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">717</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">convert_to_tensors</span>                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 714 │   │   │   │   │   </span>value = [value]                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 715 │   │   │   │   </span>                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 716 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> is_tensor(value):                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 717 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tensor = as_tensor(value)                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 718 │   │   │   │   │   </span>                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 719 │   │   │   │   │   # Removing this for now in favor of controlling the shape with</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 720 │   │   │   │   │   # # at-least2d</span>                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>too many dimensions <span style=\"color: #008000; text-decoration-color: #008000\">'str'</span>\n",
       "\n",
       "<span style=\"font-style: italic\">During handling of the above exception, another exception occurred:</span>\n",
       "\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\Remco\\AppData\\Local\\Temp\\ipykernel_19948\\1586504221.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>train_dataloader = DataLoader(preprocessed_datasets[<span style=\"color: #808000; text-decoration-color: #808000\">'train'</span>], shuffle=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, batch_siz <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>                                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6 # Now we can iterate over the dataloader to get batches of data</span>                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>7 data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">iter</span>(train_dataloader))                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">8 </span>data[<span style=\"color: #808000; text-decoration-color: #808000\">'input_ids'</span>].shape, data[<span style=\"color: #808000; text-decoration-color: #808000\">'attention_mask'</span>].shape, data[<span style=\"color: #808000; text-decoration-color: #808000\">'labels'</span>].shape           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">628</span> in  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__next__</span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 625 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._sampler_iter <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 626 │   │   │   │   # TODO(https://github.com/pytorch/pytorch/issues/76750)</span>            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 627 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._reset()  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[call-arg]</span>                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 628 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._next_data()                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 629 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._num_yielded += <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 630 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._dataset_kind == _DatasetKind.Iterable <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> \\                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 631 │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._IterableDataset_len_called <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> \\             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">671</span> in  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_next_data</span>                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 668 │   </span>                                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 669 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_next_data</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 670 │   │   </span>index = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._next_index()  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># may raise StopIteration</span>                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 671 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._dataset_fetcher.fetch(index)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># may raise StopIteration</span>       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 672 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._pin_memory:                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 673 │   │   │   </span>data = _utils.pin_memory.pin_memory(data, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._pin_memory_device)     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 674 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> data                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">61</span> in <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fetch</span>                                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">58 │   │   │   │   </span>data = [<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset[idx] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> idx <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> possibly_batched_index]         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">59 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">60 │   │   │   </span>data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset[possibly_batched_index]                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>61 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.collate_fn(data)                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">62 </span>                                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\transformers\\data\\data_collator.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">588</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 585 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 586 │   │   │   │   │   </span>feature[<span style=\"color: #808000; text-decoration-color: #808000\">\"labels\"</span>] = np.concatenate([remainder, feature[<span style=\"color: #808000; text-decoration-color: #808000\">\"labels</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 587 │   │   </span>                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 588 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>features = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.tokenizer.pad(                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 589 │   │   │   </span>features,                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 590 │   │   │   </span>padding=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.padding,                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 591 │   │   │   </span>max_length=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.max_length,                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\transformers\\tokenization_utils_base.p</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">y</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3020</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pad</span>                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3017 │   │   │   │   │   </span>batch_outputs[key] = []                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3018 │   │   │   │   </span>batch_outputs[key].append(value)                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3019 │   │   </span>                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3020 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> BatchEncoding(batch_outputs, tensor_type=return_tensors)            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3021 │   </span>                                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3022 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">create_token_type_ids_from_sequences</span>(                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3023 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, token_ids_0: List[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>], token_ids_1: Optional[List[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>]] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\transformers\\tokenization_utils_base.p</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">y</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">210</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 207 │   │   </span>                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 208 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._n_sequences = n_sequences                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 209 │   │   </span>                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 210 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepen <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 211 │   </span>                                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 212 │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@property</span>                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 213 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">n_sequences</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; Optional[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>]:                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\transformers\\tokenization_utils_base.p</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">y</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">733</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">convert_to_tensors</span>                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 730 │   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Unable to create tensor returning overflowing tokens of d</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 731 │   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Please see if a fast version of this tokenizer is availab</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 732 │   │   │   │   │   </span>)                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 733 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 734 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Unable to create tensor, you should probably activate truncat</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 735 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" 'padding=True' 'truncation=True' to have batched tensors wit</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 736 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\" features (`{</span>key<span style=\"color: #808000; text-decoration-color: #808000\">}` in this case) have excessive nesting (inp</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>Unable to create tensor, you should probably activate truncation and/or padding \n",
       "with <span style=\"color: #008000; text-decoration-color: #008000\">'padding=True'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'truncation=True'</span> to have batched tensors with the same length. Perhaps \n",
       "your features <span style=\"font-weight: bold\">(</span>`explanation_1` in this case<span style=\"font-weight: bold\">)</span> have excessive nesting <span style=\"font-weight: bold\">(</span>inputs type `list` where\n",
       "type `int` is expected<span style=\"font-weight: bold\">)</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭─\u001b[0m\u001b[91m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[91m ───────────────────────────\u001b[0m\u001b[91m─╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33mc:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\transformers\\tokenization_utils_base.p\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33my\u001b[0m:\u001b[94m717\u001b[0m in \u001b[92mconvert_to_tensors\u001b[0m                                                               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 714 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mvalue = [value]                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 715 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 716 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m is_tensor(value):                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 717 \u001b[2m│   │   │   │   │   \u001b[0mtensor = as_tensor(value)                                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 718 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m                                                               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 719 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# Removing this for now in favor of controlling the shape with\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 720 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# # at-least2d\u001b[0m                                                 \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mtoo many dimensions \u001b[32m'str'\u001b[0m\n",
       "\n",
       "\u001b[3mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
       "\n",
       "\u001b[91m╭─\u001b[0m\u001b[91m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[91m ───────────────────────────\u001b[0m\u001b[91m─╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33mC:\\Users\\Remco\\AppData\\Local\\Temp\\ipykernel_19948\\1586504221.py\u001b[0m:\u001b[94m7\u001b[0m in \u001b[92m<module>\u001b[0m             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m4 \u001b[0mtrain_dataloader = DataLoader(preprocessed_datasets[\u001b[33m'\u001b[0m\u001b[33mtrain\u001b[0m\u001b[33m'\u001b[0m], shuffle=\u001b[94mTrue\u001b[0m, batch_siz \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m5 \u001b[0m                                                                                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m6 \u001b[0m\u001b[2m# Now we can iterate over the dataloader to get batches of data\u001b[0m                       \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m7 data = \u001b[96mnext\u001b[0m(\u001b[96miter\u001b[0m(train_dataloader))                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m8 \u001b[0mdata[\u001b[33m'\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m'\u001b[0m].shape, data[\u001b[33m'\u001b[0m\u001b[33mattention_mask\u001b[0m\u001b[33m'\u001b[0m].shape, data[\u001b[33m'\u001b[0m\u001b[33mlabels\u001b[0m\u001b[33m'\u001b[0m].shape           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33mc:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m:\u001b[94m628\u001b[0m in  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[92m__next__\u001b[0m                                                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 625 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._sampler_iter \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 626 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 627 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._reset()  \u001b[2m# type: ignore[call-arg]\u001b[0m                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 628 \u001b[2m│   │   │   \u001b[0mdata = \u001b[96mself\u001b[0m._next_data()                                               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 629 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._num_yielded += \u001b[94m1\u001b[0m                                                 \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 630 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._dataset_kind == _DatasetKind.Iterable \u001b[95mand\u001b[0m \\                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 631 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._IterableDataset_len_called \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m \\             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33mc:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m:\u001b[94m671\u001b[0m in  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[92m_next_data\u001b[0m                                                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 668 \u001b[0m\u001b[2m│   \u001b[0m                                                                               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 669 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_next_data\u001b[0m(\u001b[96mself\u001b[0m):                                                          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 670 \u001b[0m\u001b[2m│   │   \u001b[0mindex = \u001b[96mself\u001b[0m._next_index()  \u001b[2m# may raise StopIteration\u001b[0m                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 671 \u001b[2m│   │   \u001b[0mdata = \u001b[96mself\u001b[0m._dataset_fetcher.fetch(index)  \u001b[2m# may raise StopIteration\u001b[0m       \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 672 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._pin_memory:                                                       \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 673 \u001b[0m\u001b[2m│   │   │   \u001b[0mdata = _utils.pin_memory.pin_memory(data, \u001b[96mself\u001b[0m._pin_memory_device)     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 674 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m data                                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33mc:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m:\u001b[94m61\u001b[0m in \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[92mfetch\u001b[0m                                                                                     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m58 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdata = [\u001b[96mself\u001b[0m.dataset[idx] \u001b[94mfor\u001b[0m idx \u001b[95min\u001b[0m possibly_batched_index]         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m59 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m60 \u001b[0m\u001b[2m│   │   │   \u001b[0mdata = \u001b[96mself\u001b[0m.dataset[possibly_batched_index]                              \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m61 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.collate_fn(data)                                                 \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m62 \u001b[0m                                                                                     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33mc:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\transformers\\data\\data_collator.py\u001b[0m:\u001b[94m588\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m in \u001b[92m__call__\u001b[0m                                                                               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 585 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                              \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 586 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mfeature[\u001b[33m\"\u001b[0m\u001b[33mlabels\u001b[0m\u001b[33m\"\u001b[0m] = np.concatenate([remainder, feature[\u001b[33m\"\u001b[0m\u001b[33mlabels\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 587 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 588 \u001b[2m│   │   \u001b[0mfeatures = \u001b[96mself\u001b[0m.tokenizer.pad(                                             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 589 \u001b[0m\u001b[2m│   │   │   \u001b[0mfeatures,                                                              \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 590 \u001b[0m\u001b[2m│   │   │   \u001b[0mpadding=\u001b[96mself\u001b[0m.padding,                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 591 \u001b[0m\u001b[2m│   │   │   \u001b[0mmax_length=\u001b[96mself\u001b[0m.max_length,                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33mc:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\transformers\\tokenization_utils_base.p\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33my\u001b[0m:\u001b[94m3020\u001b[0m in \u001b[92mpad\u001b[0m                                                                             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3017 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mbatch_outputs[key] = []                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3018 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mbatch_outputs[key].append(value)                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3019 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m3020 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m BatchEncoding(batch_outputs, tensor_type=return_tensors)            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3021 \u001b[0m\u001b[2m│   \u001b[0m                                                                               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3022 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcreate_token_type_ids_from_sequences\u001b[0m(                                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m3023 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m, token_ids_0: List[\u001b[96mint\u001b[0m], token_ids_1: Optional[List[\u001b[96mint\u001b[0m]] = \u001b[94mNone\u001b[0m      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33mc:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\transformers\\tokenization_utils_base.p\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33my\u001b[0m:\u001b[94m210\u001b[0m in \u001b[92m__init__\u001b[0m                                                                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 207 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 208 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._n_sequences = n_sequences                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 209 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 210 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepen \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 211 \u001b[0m\u001b[2m│   \u001b[0m                                                                               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 212 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@property\u001b[0m                                                                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 213 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mn_sequences\u001b[0m(\u001b[96mself\u001b[0m) -> Optional[\u001b[96mint\u001b[0m]:                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33mc:\\Users\\Remco\\PythonEnvs\\nlpenv\\lib\\site-packages\\transformers\\tokenization_utils_base.p\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33my\u001b[0m:\u001b[94m733\u001b[0m in \u001b[92mconvert_to_tensors\u001b[0m                                                               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 730 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mUnable to create tensor returning overflowing tokens of d\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 731 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mPlease see if a fast version of this tokenizer is availab\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 732 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m)                                                              \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 733 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 734 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mUnable to create tensor, you should probably activate truncat\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 735 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m \u001b[0m\u001b[33m'\u001b[0m\u001b[33mpadding=True\u001b[0m\u001b[33m'\u001b[0m\u001b[33m \u001b[0m\u001b[33m'\u001b[0m\u001b[33mtruncation=True\u001b[0m\u001b[33m'\u001b[0m\u001b[33m to have batched tensors wit\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 736 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m features (`\u001b[0m\u001b[33m{\u001b[0mkey\u001b[33m}\u001b[0m\u001b[33m` in this case) have excessive nesting (inp\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mUnable to create tensor, you should probably activate truncation and/or padding \n",
       "with \u001b[32m'\u001b[0m\u001b[32mpadding\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m'\u001b[0m \u001b[32m'\u001b[0m\u001b[32mtruncation\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m'\u001b[0m to have batched tensors with the same length. Perhaps \n",
       "your features \u001b[1m(\u001b[0m`explanation_1` in this case\u001b[1m)\u001b[0m have excessive nesting \u001b[1m(\u001b[0minputs type `list` where\n",
       "type `int` is expected\u001b[1m)\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataloader\n",
    "train_dataloader = DataLoader(datasets['train'], shuffle=True, batch_size=16, collate_fn=data_collator)\n",
    "\n",
    "# Now we can iterate over the dataloader to get batches of data\n",
    "data = next(iter(train_dataloader))\n",
    "data['input_ids'].shape, data['attention_mask'].shape, data['labels'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
