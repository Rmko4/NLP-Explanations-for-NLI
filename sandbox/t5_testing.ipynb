{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "# Parameters to be defined\n",
    "model_checkpoint = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(model_checkpoint, model_max_length=512)\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The<extra_id_0> walks in<extra_id_1> park</s>\n",
      "<extra_id_0> cute dog<extra_id_1> the<extra_id_2></s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.586868286132812"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "decoded_input = tokenizer.decode(input_ids[0])\n",
    "decoded_labels = tokenizer.decode(labels[0])\n",
    "\n",
    "print(decoded_input)\n",
    "print(decoded_labels)\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(input_ids=input_ids, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> The dog is walking in the park.</s>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(input_ids, max_length=100, num_beams=5, early_stopping=True)\n",
    "\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset esnli (C:/Users/Remco/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e584e8695040df9d46861bc2a142b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "datasets = load_dataset(\"esnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7163270711898804"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following 2 hyperparameters are task-specific\n",
    "max_source_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "# Suppose we have the following 2 training examples:\n",
    "input_sequence_1 = \"Welcome to NYC\"\n",
    "output_sequence_1 = \"Bienvenue Ã  NYC\"\n",
    "\n",
    "input_sequence_2 = \"HuggingFace is a company\"\n",
    "output_sequence_2 = \"HuggingFace est une entreprise\"\n",
    "\n",
    "# encode the inputs\n",
    "task_prefix = \"translate English to French: \"\n",
    "input_sequences = [input_sequence_1, input_sequence_2]\n",
    "\n",
    "encoding = tokenizer(\n",
    "    [task_prefix + sequence for sequence in input_sequences],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_source_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "# encode the targets\n",
    "target_encoding = tokenizer(\n",
    "    [output_sequence_1, output_sequence_2],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_target_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "labels = target_encoding.input_ids\n",
    "\n",
    "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "# forward pass\n",
    "# No need to give right shift token ids as the model automatically creates them.\n",
    "# And tokenizer add EOS token at the end of the sequence\n",
    "loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocess_function(tokenizer, training=True, max_source_length=512, max_target_length=128):\n",
    "    def _preprocess_fn(examples):\n",
    "        input_text = ['premise: ' + premise + ' \\n ' + 'hypothesis: ' + hypothesis\n",
    "                      for premise, hypothesis in zip(examples['premise'], examples['hypothesis'])]\n",
    "\n",
    "        model_inputs = tokenizer(input_text, truncation=True, max_length=max_source_length)\n",
    "\n",
    "        if training:\n",
    "            target_text = examples['explanation_1']\n",
    "            targets = tokenizer(target_text, truncation=True, max_length=max_target_length)\n",
    "\n",
    "            model_inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "        else:\n",
    "            for i in range(1, 4):\n",
    "                key_explanation = 'explanation_' + str(i)\n",
    "                target_text = examples[key_explanation]\n",
    "                targets = tokenizer(target_text, truncation=True, max_length=max_target_length)\n",
    "                model_inputs[key_explanation] = targets[\"input_ids\"]\n",
    "\n",
    "\n",
    "        return model_inputs\n",
    "    return _preprocess_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Remco\\.cache\\huggingface\\datasets\\esnli\\plain_text\\0.0.2\\a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc\\cache-d67e13d575ddf396.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Remco\\.cache\\huggingface\\datasets\\esnli\\plain_text\\0.0.2\\a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc\\cache-eb5e9201f9aa7afa.arrow\n"
     ]
    }
   ],
   "source": [
    "raw_dataset_columns = datasets['train'].column_names\n",
    "\n",
    "preprocess_function_train = get_preprocess_function(tokenizer, training=True)\n",
    "datasets['train'] = datasets['train'].map(preprocess_function_train, batched=True, remove_columns=raw_dataset_columns)\n",
    "\n",
    "preprocess_function_test = get_preprocess_function(tokenizer, training=False)\n",
    "datasets['validation'] = datasets['validation'].map(preprocess_function_test, batched=True, remove_columns=raw_dataset_columns)\n",
    "datasets['test'] = datasets['test'].map(preprocess_function_test, batched=True, remove_columns=raw_dataset_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise: A person on a horse jumps over a broken down airplane. hypothesis: A person is training his horse for a competition.</s>\n",
      "the person is not necessarily training his horse</s>\n"
     ]
    }
   ],
   "source": [
    "datasets.set_format(type='torch')\n",
    "datasets['validation'][0]\n",
    "\n",
    "print(tokenizer.decode(datasets['train'][0]['input_ids']))\n",
    "print(tokenizer.decode(datasets['train'][0]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Use data collator to create batched data. This will pad the inputs and labels to the maximum length of the batch.\n",
    "# Might be more efficient to pad to fixed length.\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True, label_pad_token_id=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 47]), torch.Size([16, 47]), torch.Size([16, 27]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataloader\n",
    "train_dataloader = DataLoader(datasets['train'], shuffle=True, batch_size=16, collate_fn=data_collator)\n",
    "\n",
    "# Now we can iterate over the dataloader to get batches of data\n",
    "data = next(iter(train_dataloader))\n",
    "data['input_ids'].shape, data['attention_mask'].shape, data['labels'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
